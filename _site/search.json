{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:01:33-05:00"
    },
    {
      "path": "index.html",
      "title": "passerium",
      "description": "A motley archive\n",
      "author": [],
      "contents": "\r\nSomehow, then, and without going mad, we must learn from these madmen to reconcile fanaticism with serenity. […] For nothing can be accomplished without fanaticism, and without serenity nothing can be enjoyed. Perfection of form or increase of knowledge, pursuit of fame or service to the community, love of God or god of Love, – we must select the Illusion which appeals to our temperament, and embrace it\r\n– A Word Cycle by Palinurus (Cyril Connolly, 1944)\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:01:40-05:00"
    },
    {
      "path": "naturalia.html",
      "title": "naturalia",
      "description": "\"items created by the earth and items drawn from nature\" - Samuel Quiccheberg (1565) Inscriptiones vel tituli theatre amplissimi\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:01:43-05:00"
    },
    {
      "path": "precip.html",
      "title": "Precipitation Scan",
      "description": "finding clusters of \"high\" and \"low\" precipitation years suitable for modelling usage\n",
      "author": [],
      "date": "December 2020",
      "contents": "",
      "last_modified": "2021-02-19T16:04:53-05:00"
    },
    {
      "path": "qin.html",
      "title": "Qin scores & history",
      "description": "also a prelude in Python\n",
      "author": [],
      "contents": "\r\nObjective - read in a webpage, find elements (score/song name, explanatory link) and create a dictionary.\r\nThe impressive source: http://silkqin.com/zh02qnpu.htm\r\nScrapping source\r\n1.extracting and renaming files\r\n2.Correctly obtaining encoded characters - since the webpage contains Chinese characters, need to ensure they are captured properly\r\nDetour INSIDE BASH (not Python)\r\npip install chardet\r\nchardetect *html #(after navigating to the correct directory, saving html file)\r\nconfirmed to be utf-8https://pypi.org/project/chardet/\r\nRefer to – https://stackoverflow.com/questions/31027759/how-to-scrape-traditional-chinese-text-with-beautifulsoup\r\n\r\nimport requests\r\nurl = 'http://silkqin.com/zh06hear.htm'\r\nresponse = requests.get(url)\r\npage_content = requests.get(url).content  # returns bytes <- this extra step allows detection for special (e.g. Chinese characters)\r\nfrom bs4 import BeautifulSoup\r\nsoup = BeautifulSoup(page_content, 'lxml')\r\n#check how it looks\r\nsoup.contents\r\n\r\nResults in: title聽絲弦古琴title\r\nmeta content=“text/html; charset=utf-8” http-equiv=“content-type”\r\nmeta content=“聽絲弦古琴” name=“description” meta content=“琴、古琴、聽琴、聽古琴、聽絲弦琴、聽絲弦古琴、絲弦、絲絃、絲線、丝弦、絲絃琴、絲弦琴、絲線琴、絲弦古琴、絲絃古琴、絲線古琴、絲桐、唐世璋、John Thompson” name=“keywords”\r\nFrom looking into the text, tre is only a loose pattern - in the location,sually “Anoottion (Rfollowed by Recording F\r\nr example, in 南風歌 （聽), the annotation page and recording are adjacent to each other and the naming is consistent.http://silkqin.com/02qnpu/10tgyy/tg01nfg.htm http://silkqin.com/06hear/myrec/1511/tg01nfg.mp3\r\nBut sometimes, the naming is not consistent, for example in 墨子悲歌 (聽）http://silkqin.com/02qnpu/32zczz/mozibei.htm http://silkqin.com/06hear/myrec/1589-1609/1609mozibeige.mp3\r\nAnd certain annotations are shorter and exists as excerpts within a page collection; no consistency in file name either, e.g. 太簇意 （聽）http://silkqin.com/02qnpu/07sqmp/sq01dsc.htm#taicouyifn http://silkqin.com/06hear/myrec/1525/xl101tcy102dhy.mp3\r\nhere is one consistent pattern though - all annotation pages seem to be under”02qnpu\" directory.\r\n\r\nimport re\r\nfrom urllib.request import urlretrieve\r\nfrom urllib.request import urlopen\r\n\r\nhtml = urlopen('http://silkqin.com')\r\nbaseurl='http://silkqin.com/'\r\n#The 'a' tag in the html does not have any text directly, but it contains a 'h3' tag that has text. \r\nall_links = [link.get(\"href\") for link in soup(\"a\")]\r\nall_links\r\n#get rid of none otherwise sub lists generated gives None type error\r\n#to read on None type - https://stackoverflow.com/questions/3887381/typeerror-nonetype-object-is-not-iterable-in-python\r\nclean = [x for x in all_links if x is not None]\r\n# now filter for that directory\r\nlinks_htm = [k for k in clean if 'htm' in k and '02qnpu' in k] \r\n#and 'htm#' not in k]#and '\\#' not in k and '\\~' not in k]\r\n\r\nThere were 164 scores listed,not 234 that results from checking length of this list.\r\nThis is likely due to the presence of lyrics being separate links, but not filter-out-able since they live in the same ‘02qnpu’ subdirectory\r\ne.g. 清商調 （聽）（看中文歌詞） http://silkqin.com/02qnpu/32zczz/daoyi.htm#qsdfn http://silkqin.com/06hear/myrec/1589-1609/1609qsdge.mp3 http://silkqin.com/02qnpu/32zczz/daoyi.htm#qsdlyr\r\nLet’s grab them all for now, knowing some are just subsections of pages (e.g. #qsdfn above) and some are lyrics\r\n\r\n#note-- don't reuse counter alink fromprevious= -> cannot force it into an integer as it becomes a list element of thelist in the for loop\r\n#trying to use it as a counter yields TypeError: list indices must be integers or slices, not str\r\ncounter = 0\r\nfor alink in links_htm:\r\n    urlretrieve((baseurl + links_htm[counter]), (links_htm[counter].rsplit('/', 1)[-1]))\r\n    #the split takes all characters after last slash\r\n    #regex way -- re.sub(r'^.+/([^/]+)$', r'\\1', 'dsf/we/sdfl.htm')\r\n    #more https://stackoverflow.com/questions/7253803/how-to-get-everything-after-last-slash-in-a-url\r\n    counter += 1\r\n\r\nResult is an error message “NameError: name ‘links_htm’ is not defined”\r\nWhat happened? Checking the directory, there are 210 of these annotation (and lyrics) html files downloaded.\r\nLet’s collect the downloaded ones (anything with htm) using glob below, and compare against the annotation links list.\r\nSince the glob collection has no subdirectory/nesting (besides the # page bookmarks), let’s strip those from the links list as well\r\n\r\nimport glob\r\ndownloadedhtmfiles = []\r\nfor file in glob.glob(\"*.htm\"):\r\n    downloadedhtmfiles.append(file)\r\n\r\nlinks_htm_temp=list(range(0,len(links_htm)))  # there will be error otherwise if list is not intialized, since we don't use append below\r\ncounter = 0\r\nfor alink in links_htm:\r\n    links_htm_temp[counter] = re.sub(r'^.+/([^/]+)$', r'\\1', links_htm[counter])\r\n    #the split takes all characters after last slash\r\n    #regex way -- re.sub(r'^.+/([^/]+)$', r'\\1', 'dsf/we/sdfl.htm')\r\n    #more https://stackoverflow.com/questions/7253803/how-to-get-everything-after-last-slash-in-a-url\r\n    counter += 1\r\n#links_htm_temp[0]\r\n\r\ndef Diff(li1, li2): \r\n    return (list(set(li1) - set(li2)))  \r\nprint(Diff(links_htm_temp, downloadedhtmfiles))\r\n\r\nOutput:\r\n[‘tg06gjq.htm#lyrchi’, ‘xl127ysc.htm#jzymusic’, ‘lh00toc.htm#p5’, ‘tingqinyin.htm#melody’, ‘daoyi.htm#qsdfn’, ‘yqwd.htm’, ‘xl028yyg.htm#chilyr’, ‘tg32cjq.htm#1525cjwt’, ‘xl132src.htm#linzhong’, ‘jiukuang.htm#chilyr’, ‘tg36kcyh.htm#music’, ‘03slgj.htm#kzhyy’, ‘tg01nfg.htm#lyrics’, ‘hw02qpy.htm’, ‘xl096yts.htm#lyrics’, ‘xl127ysc.htm#ysymusic’, ‘xl054cwy.htm#mjyfn’, ‘27wjctrans.htm#record’, ‘1709qfq.htm#1840muslyr’, ‘tg28frsg.htm#chilyr’, ‘daoyi.htm#lyrchi’, ‘tg10ysc.htm#chilyr’, ‘xl054cwy.htm#jy’, ‘qx14wywq.htm’, ‘tg32cjq.htm#clyrics’, ‘xl000toc.htm#p16’, ‘xl021fl.htm#feidianyinfn’, ‘fm23ygsd.htm#chilyr’, ‘xl098byd.htm#chilyrfn’, ‘cx38xsq.htm#lyrics’, ‘tg24hzd.htm#chilyr’, ‘zy13ygsd.htm#v1’, ‘fx33gg.htm#lyricsfn’, ‘fx42zwy.htm#chilyr’, ‘sq01dsc.htm#dinghuiyinfn’, ‘xl046yz.htm#1530’, ‘1709qfq.htm#1709muslyr’, ‘tg02sqc.htm#lyrics’, ‘daoyi.htm#qsdlyr’, ‘tg03xfy.htm#chilyr’, ‘sj03qjj.htm#chilyr’, ‘hw15fhts.htm’, ‘fx40dmyt.htm#chilyr’, ‘tg09wwq.htm#muslyr’, ‘tg32cjq.htm#1539cj’, ‘ty28skj.htm’, ‘lq12mss.htm’, ‘fm03qjwd.htm#chilyr’, ‘fx27wjc.htm#chilyr’, ‘xl041jyb.htm#qingyeyin’, ‘qx09lhxx.htm’, ‘ty28skj.htm#skjmp3’, ‘ylcx.htm#cgyfn’, ‘xl041jyb.htm#chilyr’, ‘yltrans.htm’, ‘fx32dyq.htm#chilyr’, ‘sq01dsc.htm#taicouyifn’, ‘tg16ysc.htm#chilyr’, ‘ylcx.htm#ylcxmusic’, ‘xl159qxb.htm#byyfn’, ‘sq18ghy.htm#daguanyinfn’, ‘fx45gjx.htm#xllyrfn’, ‘jiukuang.htm#lyrics’, ‘03slgj.htm#gd’, ‘fx31lsm.htm#chilyr’, ‘tg08ksc.htm#lyrics’, ‘ty6qcby.htm#gy’, ‘tg07wwc.htm#music’, ‘xl046yz.htm#chilyr’, ‘xl007gky.htm#chongheyinfn’, ‘xl159qxb.htm#qyyfn’, ‘xl155fqh.htm#chilyr’, ‘tg25gqlc.htm#chilyr’, ‘tg35gqf.htm#chilyr’, ‘sz03olwj.htm’]\r\nA dictionary to cross-reference\r\nPlaying with Regex and making the first pairing for dict\r\ntake all htm, strip out #… match blurb.htm\"> and  First let’s reset the links list in case of any accidental changes before\r\n\r\n#repeat of above code (in case run from this segment)\r\n# Find links\r\nall_links = [link.get(\"href\") for link in soup(\"a\")]\r\nall_links\r\nclean = [x for x in all_links if x is not None]\r\n#links_htm = [k for k in clean if 'htm' in k and '02qnpu' in k and 'htm#' not in k]#and '\\#' not in k and '\\~' not in k]\r\n#links_htm = [k for k in links_htm if '02qnpu' in k]\r\nlinks_htm = [k for k in clean if 'htm' in k and '02qnpu' in k]\r\n\r\nlinks_htm_clean=links_htm # if I reassign below directly in re.sub, it seems to overwrite the original as well\r\nlinks_htm_clean[1] = re.sub(r'.*\\/', r'', links_htm_clean[1]) #pat1.*pat2   any number of characters between pat1 and pat2\r\nlinks_htm_clean[1] = re.sub(r'\\#.*', r'', links_htm_clean[1]) \r\nprint(links_htm_clean[1]) \r\nprint(links_htm[3])\r\nlen(links_htm_clean)\r\n\r\nResults:\r\nyltrans.htm 02qnpu/03slgj.htm 234\r\n\r\n#throwing the tested element into a loop\r\n# links_htm = [k for k in clean if 'htm' in k and '02qnpu' in k]\r\n# gets rid of slashes and anything preceding slash\r\nlinks_htm_clean=links_htm\r\ncounter=0\r\nfor elem in links_htm_clean:\r\n    links_htm_clean[counter] = re.sub(r'.*\\/', r'', links_htm_clean[counter]) #pat1.*pat2   any number of characters between pat1 and pat2\r\n#    links_htm_clean[counter] = re.sub(r'\\#.*', r'', links_htm_clean[counter]) # works but don't want to remove the # bc sometime that signifies diff song \r\n#    print(links_htm_clean[counter]) \r\n    counter +=1\r\nlinks_htm_clean[1] = re.sub(r'\\#.*', r'', links_htm_clean[1]) \r\nprint(links_htm_clean[1]) \r\nprint(links_htm[3])\r\nlen(links_htm_clean)\r\n\r\nyltrans.htm 03slgj.htm 234\r\nThe annotation, recording and song name patterns are generally: <a href=“http://silkqin.com/02qnpu/16xltq/xl154lqy.htm”>臨邛吟<\/a> <a href=“http://silkqin.com/06hear/myrec/1525/xl154lqy.mp3”>聽<\/a>）<\/li> which means we are looking as song name, the text between xl154lqy.htm\"> & <\/a> <a href=“http://silkqin.com/06hear/myrec/1525/xl154lqy.mp3” * Let’s play with splitting this string, called astring\r\n\r\n# Variation 1 - cluster annotation / song name+record link\r\nastring='<a href=\"http://silkqin.com/02qnpu/03slgj.htm#kzhyy\">開指黃鶯吟<\/a>（<a href=\"http://silkqin.com/06hear/myrec/01tangsong/00kzhyy.mp3\">聽<\/a>'\r\n#m=re.split('(.htm\\S+?>)',astring) \r\nm=re.split('.htm\\S+?>',astring)  #cuts by end of first '>' for a list of two being the ahref annotation link, then song name & recording link\r\nn=re.sub('(.mp3)\\S+','',astring) #cuts everything from the '.mp3' onward to get rid of 聽<\/a>']\r\n#\\S = a non whitespace chara,\r\n#+ multipe \\S but add ? for as few as possible\r\n#() keeps the separator within result\r\nprint(m)\r\nprint(n) \r\n\r\n’<a href=“http://silkqin.com/02qnpu/03slgj‘, ’開指黃鶯吟（聽’ 開指黃鶯吟（<a href=”http://silkqin.com/06hear/myrec/01tangsong/00kzhyy\r\n\r\n# Variation 2 - cluster annotation+song name / record link\r\nastring='<a href=\"http://silkqin.com/02qnpu/03slgj.htm#kzhyy\">開指黃鶯吟<\/a>（<a href=\"http://silkqin.com/06hear/myrec/01tangsong/00kzhyy.mp3\">聽<\/a>'\r\n#m=re.split('(.htm\\S+?>)',astring) \r\nm=re.split('<\/a>（<a href=\"',astring) \r\nn=re.sub('(.mp3)\\S+','',astring)\r\nprint(m) #a list of two\r\nprint(n)\r\n\r\n‘開指黃鶯吟’, ’http://silkqin.com/06hear/myrec/01tangsong/00kzhyy.mp3“>聽’] 開指黃鶯吟（<a href=”http://silkqin.com/06hear/myrec/01tangsong/00kzhyy\r\nContinuing with variation 2, let’s regex out the typical patterns surrounding the song name.\r\n\r\n# Song Name: \r\no=re.sub(r'.*\\/', r'', m[0]) #first extract everything after the last / in the first element -> '03slgj.htm#kzhyy\">開指黃鶯吟'\r\np=re.split(r'\\\">', o) # then split by the \"> pattern between annotation link and song name ->['03slgj.htm#kzhyy', '開指黃鶯吟']\r\np[1] #the second element yields song name\r\n\r\n‘開指黃鶯吟’\r\np[0] # annotations\r\n‘03slgj.htm#kzhyy’\r\nBuild the dictionary\r\nRecall how to set up a dictionary. Then fit in song name and file name extracted above (and hope the pattern holds)\r\nRecCatalogue={} RecCatalogue={p[1]:r.group(0)} RecCatalogue\r\n\r\n#print the soup as string text for use \r\nSsoup=str(soup)\r\nprint(Ssoup,  file=open('Ssoup.txt', 'w',encoding='utf-8-sig'))\r\n#for song titles,extract from the mass of text in SSoup file\r\n#sub-example \"<br/><a href=\"06hear/myrec/1491/zy08ygd.mp3\"><b>聽漁歌調<\/b><\/a>\"\r\ncounter=0\r\nsep=[i for i in links_rec if i in Ssoup]\r\nfor elem in sep:\r\n    sep = [i for i in links_rec if i in Ssoup]\r\n    lsep=len(sep[counter]) #length of the recording file name\r\n    idx = Ssoup.find(sep[counter]) #note where the recordingfile name is in the Ssoup string\r\n    _idx=idx-lsep #set starting index back the length of recording file name\r\n    TitleName[counter]=(Ssoup[_idx:idx-12])\r\n    TitleName[counter]=TitleName[counter].split(sep=\">\",maxsplit=1)[1] # cut everything before <b> which precedes title\r\n    TitleName[counter]=TitleName[counter].split(sep=\"<\/\",maxsplit=1)[0] # cut everything behind <\/b> which follows title, keeping first element (Title)\r\n    counter+=1\r\n\r\nCheck if we obtained the name\r\nTitleName[19] ‘廣寒秋’\r\n\r\n#piecemeal testing looking for explanation htm files which just precede song titles\r\nteststr=\"<a href=\\\"balshblahs<a href=\\\"http://silkqin.com/02qnpu/32zczz/tingqinyin.htm\\\">聽琴吟<\/a>\"\r\nteststr\r\nidx = Ssoup.find(\"古風操\")\r\nidx\r\n\r\n5347\r\n\r\nidx_ = Ssoup[idx-180:idx].rfind('<a href=\\\"') #reverse find looks forward, so set it some number back from index\r\nprint(Ssoup[idx-180+idx_:idx])\r\n\r\n\r\n\r\no=re.sub(r'.*\\/', r'', teststr)\r\no\r\n\r\n‘tingqinyin.htm’\r\n\r\n# trying this in the text string Ssoup \r\nidx = Ssoup.find(elem) #note where the file name is in the Ssoup string #'秋風辭'\r\nidx_ = Ssoup[idx-50:idx].rfind('<a href=\\\"') \r\nidx_=idx-50+idx_\r\nidx_\r\nSsoup[idx_:idx]\r\nteststr=Ssoup[idx_+9:idx-2]\r\nteststr=re.sub(r'.*\\/', r'', teststr)\r\nteststr\r\n\r\n‘ylcx.htm#cgyfn’\r\nReadying for loop by setting up htm list “explan” and cutting out first element which is a blank for some reason\r\n\r\nTitleName=TitleName[1:]\r\nexplan=TitleName\r\n#alternatively extract title names from previously compiled dictionary:RecCatalogue.keys()\r\n\r\nlooping this search\r\n\r\n#example \"<br/>\"<a href=\\\"http://silkqin.com/02qnpu/32zczz/tingqinyin.htm\\\">聽琴吟<\/a>\"\r\nteststr=\"blob\"\r\n#remSsoup=Ssoup\r\nsep = [i for i in TitleName if i in Ssoup]\r\ncounter=0\r\nfor elem in sep:\r\n    sep = [i for i in TitleName if i in Ssoup]\r\n    idx = Ssoup.find(elem) #find where songtitle is\r\n    idx_ = Ssoup[idx-50:idx].rfind('<a href=\\\"') #then skip 50 characters back and look for match of a href link with highest index (closest to song title)\r\n    idx_=idx-50+idx_\r\n    teststr=Ssoup[idx_+9:idx-2] #cut out the a href frames brackets\r\n    explan[counter]=re.sub(r'.*\\/', r'', teststr) # get rid of everything before last slash in htm link\r\n    counter+=1\r\nexplan\r\n\r\nOutputt explanatory htm’s bby song\r\npyt{python, eval=FALSE,include=TRUE} #HtmCatalogue=dict(zip(TitleName, explan)) HtmCatalogue ``` hSuccessful output:\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:04:58-05:00"
    },
    {
      "path": "scrappatch.html",
      "title": "Scrap patch",
      "description": "A temporary berth\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:05:05-05:00"
    },
    {
      "path": "scratchpad.html",
      "title": "Scratch pad",
      "description": "*Scribble in Markdown for practice*\n",
      "author": [],
      "contents": "\r\nStill some distance from Pelee, the vast snow-laden farm-plains slip behind, and sparse dwellings accrete until they border both sides of the road, blotting up the white-on-blue.\r\nFlitting between houses, the opaque white yonder also whittles out. Stolen glimpses of silvered sparks alight the south - the iced lake. Spangling in frosted stasis, inumerable light shards strewed over the frozen expanse through to the paled horizon.A Milky Way descended, glinting fiercer in sunlit white than midnight monochrome.\r\nSo this is where stars bask and repose, radiant.\r\nThe way south to the point extruding to the lake is barely touched by snow, sheltered on either side, unexpectedly, by trees staking a strong stand on a wisp of land so tenuous on the map. It is winter, so one may drive nearly out to the tip-point. An observation tower stands at the footpath’s start, but is of such heft and jarringness (an artifice foreign and irreducible) that it is an absurdity soon negated. (Though not meant in total slight to its being - alighting it on the way back reveals a telling perspective) The brambling trees end abruptly into a staid blue, hardly astir through to the soft-hued horizon.\r\nTurning west, the tree veil recedes to reveal juxtaposing chaos. A continuous crest of massive ice-flats - shards of giant’s mirror, or a chandelier perhaps - each larger than a raft, piled-up and hedging the entire length of the windward shore. Caspar David Friedrich’s Polar Sea , in greater subtlety, alit and pristine.\r\n\r\nCaspar David Friedrich, EismeerSource & seen at: Hamburger Kunsthalle\r\nIt had barely snowed since the morning, and the haphazardly angled ice-slats were flawlessly lucent. Ice of such scarce time, such youth, it has taken no colour of the world (and will never, here, accrue the time-density of glacial ethereal hue). The shore-shattered shards lay as a multitde of white down’d slates. What nature tosses up carelessly at flotsams we can only perceive in awe, and conceive in much diminished forms as aping lapidaries. The inhuman beauty, in its glassy denseness and brilliant intensity, envelopes and negates us.\r\n\r\nWith frost nipping nose and fingers, a hasty ascent up the tower for a parting shot.\r\n\r\nBibliography\r\nCamus, Albert. The Myth of Sisyphus trans. Justin O’Brien (London: Hamish Hamilton, 1973)\r\nMacfarlane,Robert. Underland (London: Hamish Hamilton, 2019)\r\n董其昌.various works. National Palace Museum. https://theme.npm.edu.tw/exh105/dongqichang/ch/index.html\r\nFriedrich,Caspar David. Das Eismeer. Hamburger Kunsthalle https://www.hamburger-kunsthalle.de/die-kunst-ist-offentlich-deutsch\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:05:08-05:00"
    },
    {
      "path": "shivermetimbers.html",
      "title": "Shiver Me Timbers",
      "description": "Trialling Tableau to present open source forest monitoring and climate data in dashboard form\n",
      "author": [
        {
          "name": "MECP hackathon team 3",
          "url": "https://jjyh.github.io/hackathon-OFBN"
        }
      ],
      "contents": "\r\nThe Look\r\nFeatures\r\ninteractive manipulation of the forest Decline Index formula\r\nmapped forest plots and climate stations\r\neco-regional trending References, approach and data pre-processing documentation\r\n\r\n\r\nLessons Learnt (personal opinions)- the platform\r\nDOA outside of specific server environment (not standalone file), very slow to render\r\nno provision for collaboration or version control in this version\r\nneeded heavy wrangling outside of software esp. for GIS\r\nGUI presents a challenge to share/reproduce, requiring animated GIF captures\r\na lot of “tricking” the system required, lack of fine control and unclear parsing (overcome by training?)\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:05:11-05:00"
    },
    {
      "path": "wysiwyg.html",
      "title": "WYSIWYG?",
      "author": [],
      "contents": "\r\n@ https://github.com/jjyh\r\n Source: 第19回東京大学制作展 “WYSIWYG?”\r\n\r\n\r\n\r\n",
      "last_modified": "2021-02-19T16:05:13-05:00"
    }
  ],
  "collections": []
}
